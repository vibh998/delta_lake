{"cells":[{"cell_type":"markdown","source":["###common to all stages\n\nlogging table shape \n\nprocess_id string not null ,\nbatchID string not null ,\nfile_name string not null ,\nfile_table_size string,\nfile_table_import_status string,\nfile_table_records bigint,\nfile_error_records bigint,\ningestion_timestamp timestamp,\nload_comment string,\ncurrent_dttm timestamp"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ac9f97a1-e35f-4d52-8737-889a3bdb1205","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import os\nimport pathlib\nimport fnmatch\nimport pandas as pd\nimport boto3\nimport delta\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import input_file_name\nimport pyspark.sql.types as T\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType , DateType\nfrom datetime import datetime\nfrom delta.tables import DeltaTable\n\n\n\"\"\"Class will be implemented for each batch. Read each batch data and perform A) For each batch calculate record count\nB) get file name C  ) write to logger table for each batch\"\"\"\n\nclass deltawriter:\n  validation_seq = list()\n  method_map = dict()\n  def __init__(self,batchdf, batchID, processid , sink_delta_table, dq_table , validation_seq, method_map , groupbykey , count_col):\n    \"\"\"TBD\"\"\"\n    self.validation_seq = validation_seq\n    self.method_map = method_map\n    self.batchdf = batchdf\n    self.batchID = batchID\n    self.processid = processid\n    self.sink_delta_table = sink_delta_table\n    self.dq_table = dq_table\n    #self.batchdf.createOrReplaceGlobalTempView(\"batchData\")\n    self.dq_config = spark.sql(f\"select * from {self.dq_table}\").filter(f\"Active = 'y' and process_id = '{self.processid}'\").toPandas()\n    #print(self.dq_config)\n    self.prim_keys = self.dq_config.loc[self.dq_config[\"is_primary_key\"] == 'y'].col_name.tolist()\n    #print(self.prim_keys )\n    self.non_nullable_keys = self.dq_config.loc[self.dq_config[\"is_required\"] == 'y'].col_name.tolist()\n    self.parse_config_rules()\n    \n  def create_register_delta(self , stage):\n        raw_schema = (\n          self.batchdf.schema\n        )\n        source_delta_table = self.sink_delta_table.split('.')[1:][0]\n        if not DeltaTable.isDeltaTable(spark,'/user/hive/warehouse/'+ str (source_delta_table.replace('_bronze',f\"_{stage}\"))):\n            emptyDF = spark.createDataFrame(spark.sparkContext.emptyRDD(), raw_schema)          \n            emptyDF.write.format('delta').mode('overwrite')\\\n            .save('/user/hive/warehouse/'+ str (source_delta_table.replace('_bronze','_silver')))\n            \n            #register table\n            silver_table = str(source_delta_table.replace('_bronze','_silver'))\n            silver_location = '/user/hive/warehouse/'+ silver_table\n            spark.sql(f\"CREATE TABLE {silver_table} USING DELTA LOCATION '{silver_location}'\")\n            \n    \n  def append_logging_delta(self):\n      df_count =  self.batchdf.count()\n      log_df = self.batchdf.select(F.lit(self.processid).alias(\"processid\") , \n                         F.lit(self.batchID).alias(\"batchID\"),\n                         F.col('_filename').alias(\"filename\"),\n                         F.lit(self.sink_delta_table).alias(\"filesize\"),\n                         F.lit('processed').alias(\"file_table_import_status\"),\n                         F.lit(df_count).alias(\"filecount\") , \n                         F.lit(0).alias(\"file_error_records\"),  ##to be updated in silver stage\n                         F.col ('_execute_timestamp').alias(\"ingestion_timestamp\"),\n                         F.lit('NA').alias('comment'),\n                         F.lit(datetime.now()).cast(T.TimestampType()).alias(\"current_timestamp\")\n                        )\n      (\n        log_df.dropDuplicates().write.format(\"delta\")\n        .mode(\"append\")#|\"overwrite\"\n        #.partitionBy(\"date\") # optional\n        .option(\"mergeSchema\", \"true\") # option - evolve schema\n        .saveAsTable(\"streaming_log\") #| .save(\"/path/to/delta_table\")\n      )\n\n  def parse_config_rules(self):\n    config_df = spark.sql(f\"select * from {self.dq_table}\").filter(f\"Active = 'y' and process_id = '{self.processid}'\").toPandas()\n    #display(config_df)\n\n    config_dict = {x:eval(y) for x,y in zip(config_df.col_name,config_df.formatting_rules)}\n    #print(config_dict)\n\n    key_set = {key for sub_keys in map(dict.values, config_dict.values()) for key in sub_keys}\n    #print(key_sets)\n\n    self.dq_config = {key:[col for col in config_dict if key in config_dict[col].values()] for key in key_set }\n\n  def deduplicate_merge_silver(self, df):\n    delta_sink = self.sink_delta_table.replace(\"_bronze\",\"_silver\").replace(\"default.\",\"\")\n    deltaTableSilver = DeltaTable.forPath(spark, 'dbfs:/user/hive/warehouse/'+str(delta_sink))\n    join_condition = ' AND'.join([f\" silver.{key} = updates.{key}\" for key in self.prim_keys])\n    drop_duplicates = ' ,'.join([f\"{key}\" for key in self.prim_keys])\n    df = df.dropDuplicates(self.prim_keys)\n    deltaTableSilver.alias('silver').merge(\\\n    df.alias('updates'), f\"{join_condition}\").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n\n  def writeDeltaBronze(self):\n    self.batchdf.write.format('delta')\\\n        .mode('append')\\\n        .option(\"mergeSchema\", \"true\")\\\n         .saveAsTable(self.sink_delta_table)\n    \n\n\n\n  def deduplicate_merge_gold(self, df ):\n    groupbykey = self.groupbykey\n    count_col = self.count_col\n    \n    df = df.select(F.expr(f\"date({groupbykey})\").alias(groupbykey)).groupby(groupbykey).count()\n    \n    df =  df.withColumn(groupbykey, F.col(groupbykey).cast(DateType()))\\\n    .withColumn(\"count\", F.col(\"count\").cast(IntegerType()))\n    \n    source_delta_table = self.sink_delta_table.split('.')[1:][0].replace('_bronze',\"_gold\").replace(\"default.\",\"\")\n    spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {source_delta_table}\n    ({groupbykey}  date , count int  )\"\"\")\n    join_condition = f\"gold.{groupbykey} == aggregates.{groupbykey}\"\n    deltaTableGold = DeltaTable.forPath(spark, 'dbfs:/user/hive/warehouse/'+str(source_delta_table))\n    deltaTableGold.alias('gold')\\\n    .merge(\n      df.alias('aggregates'),\n      f\"{join_condition}\"\\\n    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n    \n\n  \n  def pk_check(self):\n    df = self.batchdf\n    for key in (self.prim_keys):\n      if key not in df.columns:\n        df = df.withColumn(\"_record_type\", F.lit(\"invalid\"))\n    return df\n\n    \n  def null_check(self,df):      #check required fields\n    required_fields =self.non_nullable_keys\n    #dict_null = {field : df.filter(f\"{field} is null\").count() if df.filter(f\"{field} is null\").count() > 0 for field in required_fields}  \n    for field in required_fields:\n        df = df.withColumn(\"_record_type\", F.when(F.col(field).isNull(),\"invalid\").otherwise(\"silver\"))         \n    return df  \n  \n  def modify_execute_timestamp(self, df):\n    #modify execcute timestamp\n    df = df.withColumn(\"_execute_timestamp\", F.lit(datetime.now()).cast(T.TimestampType()))\n    return df\n\n  def apply_dq_checks(self, df):\n    \n    for trans in self.validation_seq:\n      for colm in self.dq_config[trans]:\n        df = getattr(self, self.method_map[trans])(df,colm) # (dq_config[trans])\n    return df\n    \n  def to_upper(self,df, col_name):\n    df = df.withColumn(col_name , F.upper(F.col(col_name)).alias(col_name.strip()))\n    return df\n\n  def to_convert_dttm_format(self, df, col_name):\n    df = df.withColumn(col_name+str(\"_cnv\"), F.date_format(F.col(col_name).cast(T.StringType()), 'dd-MMM-yyyy HH:MM:SS'))\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"11ee8bcd-51a4-40d8-b09a-183dcf0b45e3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f75ff6a9-4bdf-4260-8018-03721e5f402a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"unified_delta_writer","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2729734662565408}},"nbformat":4,"nbformat_minor":0}
