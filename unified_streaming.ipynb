{"cells":[{"cell_type":"markdown","source":["config table -\n1. Process_id\n2. Source_bucket_path\n3. Source_file_format \n4. Source_file_pattern\n5. Sink_delta_table\n6. Column_delimeter\n7. Preprocessing_steps - Json string   - unzip , pgp etc\n8. Min_number_of_records\n9. Max_latency_hours\n10. Active\n11. checkpoint location"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"22eb5603-11ea-4516-920a-883f0482d12d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["##readStream Options\n\nspark_read_options = {\n  \"inferSchema\" : True,\n  \"sep\" : ',' ,\n  \"header\" :  True ,\n  \"multiLine\" : True ,\n  \"maxFilesPerTrigger\" : 1,\n  \"mergeSchema\": True\n}\n\nspark_write_options = {  \n  \"OutputMode\" : \"append\",\n  \"format\" : \"delta\",\n}"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"494949d6-9d2b-4fc2-b837-550cb28f9c65","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#set spark merge schema property to true\nspark.sql(f\"SET spark.databricks.delta.schema.autoMerge.enabled = true\") "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8ee437b2-f132-4285-87de-2de59d171aaa","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[4]: DataFrame[key: string, value: string]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[4]: DataFrame[key: string, value: string]"]}}],"execution_count":0},{"cell_type":"code","source":["%run /vs_unified_streaming-Test/unified_delta_writer"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"acee9506-c84b-4a57-87ed-25420b6c5fe6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import os\nimport pathlib\nimport fnmatch\nimport pandas as pd\nimport boto3\nimport delta\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import input_file_name\nimport pyspark.sql.types as T \nfrom datetime import datetime\n\nclass Process(deltawriter):\n    \n    #validation_seq = ['UPPER', ]#'WITHOUT_SPECIAL_CHARS', 'NUMBER_FORMAT', 'CONVERT_DTTM_FORMAT']\n    \n\n    def __init__(self, process_id , dq_table):\n    #read active configurations\n        config_list = spark.sql(f\"select * from {config_delta_table}\").filter(\"active == 'y'\").filter(f\"process_id == '{process_id}'\").collect()[0]\n        self.validation_seq = ['UPPER','CONVERT_DTTM_FORMAT' ]#'WITHOUT_SPECIAL_CHARS', 'NUMBER_FORMAT']\n        self.method_map = {'UPPER': 'to_upper', 'WITHOUT_SPECIAL_CHARS': 'to_without_special_chars', 'NUMBER_FORMAT': 'to_number_format', 'CONVERT_DTTM_FORMAT': 'to_convert_dttm_format'}\n        ##set class properties\n        self.process_id = process_id\n        self.source_bucket_path = config_list.source_bucket_path\n        self.source_file_format = config_list.source_file_format\n        self.source_file_pattern = config_list.source_file_pattern\n        self.sink_delta_table = config_list.sink_delta_table\n        self.column_delimeter = config_list.column_delimeter\n        self.groupbykey = config_list.groupbykey\n        self.count_col = config_list.count_col\n        #self.preprocessing_steps - Json string   - unzip , pgp etc\n        self.preprocessing_steps = config_list.preprocessing_steps\n        self.min_number_of_records = config_list.min_number_of_records\n        self.max_latency_hours = config_list.max_latency_hours\n        self.checkpoint = config_list.checkpoint\n        self.input_schema = self.gen_schema()\n        self.dq_table = dq_table\n        \n        #print(self.input_schema)\n    \n    #@staticmethod\n    def gen_schema(self ):\n        print(self.source_file_pattern)\n#         sample_file = dbutils.fs.ls(f\"{self.source_bucket_path}{self.source_file_pattern}\")[0]\n#         schema_olist_order_items =  spark.read.csv(sample_file, header = True)\n        return (spark.read.load(f\"{self.source_bucket_path}{self.source_file_pattern}\", header='true', format='csv', inferSchema='true').schema \n        )\n            \n    def trigger_deltawriter(self,batchDf , batchID):\n        super().__init__(batchDf , batchID , self.process_id , self.sink_delta_table , self.dq_table,self.validation_seq, self.method_map , self.groupbykey , self.count_col)\n        #Bronze\n        self.batchdf.persist()    \n        self.append_logging_delta()\n        self.writeDeltaBronze()\n        self.create_register_delta(\"silver\")\n        \n        #silver\n        #df = spark.sql(f\"select * from global_temp.batchData\")\n        #df = self.modify_execute_timestamp(df, \"silver\")\n        df = self.pk_check()\n        df = self.null_check(df)\n        df = self.modify_execute_timestamp(df)\n        #df.createOrReplaceGlobalTempView(\"batchData\")\n        df = self.apply_dq_checks(df)\n        #df.createOrReplaceGlobalTempView(\"batchData\")\n        self.deduplicate_merge_silver(df)\n        self.deduplicate_merge_gold(df)\n        self.batchdf.unpersist()\n        \n\n    \n    def trigger_streams(self):\n        input_stream_df  = (\n          spark\n          .readStream\n          .format(self.source_file_format)\n          .schema(self.input_schema)  #genschema\n          .options(**spark_read_options)\n          .load(self.source_bucket_path+self.source_file_pattern)\n          #.select(\"*\",\"_metadata.file_name\", \"_metadata.file_size\")\n          .select('*', input_file_name().alias(\"_filename\") ,\\\n                  F.expr(\"uuid()\").alias(\"_unique_id\"),\\\n                  F.lit(datetime.now()).cast(T.TimestampType()).alias (\"_execute_timestamp\"),\n                  F.lit(\"bronze\").cast(T.StringType()).alias(\"_record_type\"),\n                 )\n        )\n        \n        query = (  \n          input_stream_df     #add metadata column\n          .writeStream\n          .foreachBatch(self.trigger_deltawriter)\n          .queryName(self.process_id)\n          #.format(spark_write_options['format'])\n          #.outputMode(spark_write_options['OutputMode'])\n          .option( \"checkpointLocation\", os.path.join (self.checkpoint, \"bronze\", self.process_id))\n        )\n        #query.toTable(self.sink_delta_table)\n        #query.start()\n        return (query , input_stream_df)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d4d00de4-0b0f-450c-91c6-2ca3e0e85488","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"unified_streaming","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2729734662565422}},"nbformat":4,"nbformat_minor":0}
